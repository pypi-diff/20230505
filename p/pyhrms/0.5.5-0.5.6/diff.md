# Comparing `tmp/pyhrms-0.5.5.zip` & `tmp/pyhrms-0.5.6.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 68219 bytes, number of entries: 15
-drwxrwxrwx  2.0 fat        0 b- stor 23-Mar-23 15:04 pyhrms-0.5.5/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms.egg-info/
--rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.5.5/License.txt
--rw-rw-rw-  2.0 fat    23395 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/PKG-INFO
--rw-rw-rw-  2.0 fat    22486 b- defN 23-Mar-23 15:03 pyhrms-0.5.5/README.rst
--rw-rw-rw-  2.0 fat       42 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/setup.cfg
--rw-rw-rw-  2.0 fat      899 b- defN 23-Mar-23 15:01 pyhrms-0.5.5/setup.py
--rw-rw-rw-  2.0 fat   184821 b- defN 23-Mar-23 14:38 pyhrms-0.5.5/pyhrms/pyhrms.py
--rw-rw-rw-  2.0 fat     1311 b- defN 23-Mar-13 13:39 pyhrms-0.5.5/pyhrms/__init__.py
--rw-rw-rw-  2.0 fat        1 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms.egg-info/dependency_links.txt
--rw-rw-rw-  2.0 fat    23395 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms.egg-info/PKG-INFO
--rw-rw-rw-  2.0 fat      157 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms.egg-info/requires.txt
--rw-rw-rw-  2.0 fat      216 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms.egg-info/SOURCES.txt
--rw-rw-rw-  2.0 fat        7 b- defN 23-Mar-23 15:04 pyhrms-0.5.5/pyhrms.egg-info/top_level.txt
-15 files, 257818 bytes uncompressed, 66173 bytes compressed:  74.3%
+Zip file size: 72960 bytes, number of entries: 15
+drwxrwxrwx  2.0 fat        0 b- stor 23-May-05 08:13 pyhrms-0.5.6/
+drwxrwxrwx  2.0 fat        0 b- stor 23-May-05 08:13 pyhrms-0.5.6/pyhrms/
+drwxrwxrwx  2.0 fat        0 b- stor 23-May-05 08:13 pyhrms-0.5.6/pyhrms.egg-info/
+-rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.5.6/License.txt
+-rw-rw-rw-  2.0 fat    23109 b- defN 23-May-05 08:13 pyhrms-0.5.6/PKG-INFO
+-rw-rw-rw-  2.0 fat    22200 b- defN 23-May-05 08:12 pyhrms-0.5.6/README.rst
+-rw-rw-rw-  2.0 fat       42 b- defN 23-May-05 08:13 pyhrms-0.5.6/setup.cfg
+-rw-rw-rw-  2.0 fat      899 b- defN 23-May-05 08:12 pyhrms-0.5.6/setup.py
+-rw-rw-rw-  2.0 fat   214728 b- defN 23-May-05 08:08 pyhrms-0.5.6/pyhrms/pyhrms.py
+-rw-rw-rw-  2.0 fat     1311 b- defN 23-Mar-13 13:39 pyhrms-0.5.6/pyhrms/__init__.py
+-rw-rw-rw-  2.0 fat        1 b- defN 23-May-05 08:13 pyhrms-0.5.6/pyhrms.egg-info/dependency_links.txt
+-rw-rw-rw-  2.0 fat    23109 b- defN 23-May-05 08:13 pyhrms-0.5.6/pyhrms.egg-info/PKG-INFO
+-rw-rw-rw-  2.0 fat      157 b- defN 23-May-05 08:13 pyhrms-0.5.6/pyhrms.egg-info/requires.txt
+-rw-rw-rw-  2.0 fat      216 b- defN 23-May-05 08:13 pyhrms-0.5.6/pyhrms.egg-info/SOURCES.txt
+-rw-rw-rw-  2.0 fat        7 b- defN 23-May-05 08:13 pyhrms-0.5.6/pyhrms.egg-info/top_level.txt
+15 files, 286867 bytes uncompressed, 70914 bytes compressed:  75.3%
```

## zipnote {}

```diff
@@ -1,46 +1,46 @@
-Filename: pyhrms-0.5.5/
+Filename: pyhrms-0.5.6/
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms/
+Filename: pyhrms-0.5.6/pyhrms/
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms.egg-info/
+Filename: pyhrms-0.5.6/pyhrms.egg-info/
 Comment: 
 
-Filename: pyhrms-0.5.5/License.txt
+Filename: pyhrms-0.5.6/License.txt
 Comment: 
 
-Filename: pyhrms-0.5.5/PKG-INFO
+Filename: pyhrms-0.5.6/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.5.5/README.rst
+Filename: pyhrms-0.5.6/README.rst
 Comment: 
 
-Filename: pyhrms-0.5.5/setup.cfg
+Filename: pyhrms-0.5.6/setup.cfg
 Comment: 
 
-Filename: pyhrms-0.5.5/setup.py
+Filename: pyhrms-0.5.6/setup.py
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms/pyhrms.py
+Filename: pyhrms-0.5.6/pyhrms/pyhrms.py
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms/__init__.py
+Filename: pyhrms-0.5.6/pyhrms/__init__.py
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms.egg-info/dependency_links.txt
+Filename: pyhrms-0.5.6/pyhrms.egg-info/dependency_links.txt
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms.egg-info/PKG-INFO
+Filename: pyhrms-0.5.6/pyhrms.egg-info/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms.egg-info/requires.txt
+Filename: pyhrms-0.5.6/pyhrms.egg-info/requires.txt
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms.egg-info/SOURCES.txt
+Filename: pyhrms-0.5.6/pyhrms.egg-info/SOURCES.txt
 Comment: 
 
-Filename: pyhrms-0.5.5/pyhrms.egg-info/top_level.txt
+Filename: pyhrms-0.5.6/pyhrms.egg-info/top_level.txt
 Comment: 
 
 Zip file comment:
```

## Comparing `pyhrms-0.5.5/License.txt` & `pyhrms-0.5.6/License.txt`

 * *Files identical despite different names*

## Comparing `pyhrms-0.5.5/PKG-INFO` & `pyhrms-0.5.6/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.5.5
+Version: 0.5.6
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,18 +22,18 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-Mar.23.2023: pyhrms 0.5.5 new features:
-
-    * add sat_intensity functions paramter in peak_picking fuction. The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+Mar.23.2023: pyhrms 0.5.6 new features:
 
+    * Split in different direction (RT&m/z)
+    * Can process FT_ICRMS data
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
```

## Comparing `pyhrms-0.5.5/README.rst` & `pyhrms-0.5.6/README.rst`

 * *Files 2% similar despite different names*

```diff
@@ -7,18 +7,18 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-Mar.23.2023: pyhrms 0.5.5 new features:
-
-    * add sat_intensity functions paramter in peak_picking fuction. The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+Mar.23.2023: pyhrms 0.5.6 new features:
 
+    * Split in different direction (RT&m/z)
+    * Can process FT_ICRMS data
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
```

## Comparing `pyhrms-0.5.5/setup.py` & `pyhrms-0.5.6/setup.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 def readme_file():
     with open('README.rst') as rf:
         return rf.read()
 
 setuptools.setup(
     name = 'pyhrms',
-    version = '0.5.5',
+    version = '0.5.6',
     author = 'Wang Rui',
     author_email = 'wtrt7009@gmail.com',
     url = 'https://github.com/WangRui5/PyHRMS.git',
     description = 'A powerful GC/LC-HRMS data analysis tool',
     long_description = readme_file(),
     packages = setuptools.find_packages(),
     install_requires = ['numpy>=1.19.2','pandas>=1.3.3'
```

## Comparing `pyhrms-0.5.5/pyhrms/pyhrms.py` & `pyhrms-0.5.6/pyhrms/pyhrms.py`

 * *Files 10% similar despite different names*

```diff
@@ -16,14 +16,15 @@
 from scipy.stats import ttest_ind_from_stats
 from sklearn.decomposition import PCA
 from sklearn import preprocessing
 import scipy.stats as st
 from scipy import integrate
 import itertools
 import bisect
+import re
 
 """
 ========================================================================================================
 1. basic function
 ========================================================================================================
 """
 
@@ -32,42 +33,297 @@
           'Oiso': 17.999159, 'F': 18.998403, 'K': 38.963708, 'P': 30.973763, 'Cl': 34.968853,
           'Cliso': 36.965903, 'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770,
           'Si': 27.976928, 'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
           'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486
           })
 
 
+
+def one_step_process(path, company, profile=True, p_value=1, ms2_analysis=True, fold_change=0,
+                  area_threshold=200, filter_type=3, split_n = 20,sat_intensity = False, long_rt_split_n = 1):
+    
+    """
+    This function using one processor to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
+    Args:
+       - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
+       - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
+       - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
+       - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+       - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
+       - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
+       - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
+       - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
+       - split_n (int): The number of pieces to split the large dataframe.
+       - sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+
+    returns:
+        None.Generate Excel files that summarizes the differences between the control sets and sample sets.
+
+    """
+    print('                                                                            ')
+    print('============================================================================')
+    print('First process started...')
+    print('============================================================================')
+    print('                                                                            ')
+    
+    files_mzml = glob(os.path.join(path, '*.mzML'))
+    files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
+    files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
+    for file in files_mzml:
+        first_process(file, company = company, profile=profile, ms2_analysis = ms2_analysis, 
+                      split_n = split_n, sat_intensity = sat_intensity,long_rt_split_n = long_rt_split_n)
+    
+    # 检查是否有遗漏的
+    files_excel_temp = glob(os.path.join(path, '*.xlsx'))
+    files_excel_names = [os.path.basename(i)[:-5] for i in files_excel_temp]
+    path_omitted = []
+    if len(files_mzml) > len(files_excel_names):
+        # 检查是哪个文件漏掉了
+        for path1 in files_mzml:
+            if os.path.basename(path1)[:-5] in files_excel_names:
+                pass
+            else:
+                path_omitted.append(path1)
+    if len(path_omitted) == 0:
+        pass
+    else:
+        for file in path_omitted:
+            first_process(file, company = company, profile=profile, ms2_analysis = ms2_analysis, 
+                      split_n = split_n, sat_intensity = sat_intensity, long_rt_split_n = long_rt_split_n)
+
+    # 中间过程
+    files_excel = glob(os.path.join(path, '*.xlsx'))
+    peak_alignment(files_excel)
+    ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
+    
+    
+    # 第二个过程
+    print('                                                                            ')
+    print('============================================================================')
+    print('Second process started...')
+    print('============================================================================')
+    print('                                                                            ')
+    for file in files_mzml:
+        second_process(file, ref_all, company, profile = profile,long_rt_split_n = long_rt_split_n)
+    
+    
+    # 第三个过程, 做fold change filter
+    print('                                                                            ')
+    print('============================================================================')
+    print('Third process started...')
+    print('============================================================================')
+    print('                                                                            ')
+    if filter_type == 1:
+        fold_change_filter(path, fold_change=fold_change, area_threshold=area_threshold)
+    elif filter_type == 2:
+        fold_change_filter2(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
+    elif filter_type == 3:
+        fold_change_filter3(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
+
+    # 如果有DDA，将DDA数据加入到excel里
+    files_excel = glob(os.path.join(path, '*.xlsx'))
+    unique_cmps = [file for file in files_excel if 'unique_cmps' in os.path.basename(file)]
+    for file in files_mzml_DDA:
+        df2 = gen_DDA_ms2_df(file, company, profile=profile, opt=False)
+        name = os.path.basename(file).replace('-DDA', '').replace('_DDA', '').replace('.mzML', '')  # 获得DDA文件的特征名称
+        for file_excel in unique_cmps:
+            if name in os.path.basename(file_excel):
+                df1 = pd.read_excel(file_excel)
+                for i in range(len(df1)):
+                    rt, mz = df1.loc[i, ['rt', 'mz']]
+                    df_frag = df2[(df2['precursor'] >= mz - 0.015) & (df2['precursor'] <= mz + 0.015)
+                                  & (df2['rt'] >= rt - 0.1) & (df2['rt'] <= rt + 0.1)]
+                    if len(df_frag) == 0:
+                        df1.loc[i, 'frag_DDA'] = str([])
+                    else:
+                        frag_all = []
+                        for j in range(len(df_frag)):
+                            mz1, intensity1 = df_frag['frag'].iloc[j], df_frag['intensity'].iloc[j]
+                            frag_s = pd.Series(data=intensity1, index=mz1, dtype=float)
+                            frag_all.append(frag_s)
+                        frag_s_all = pd.concat(frag_all).sort_values(ascending=False)
+                        frag_s_all1 = frag_s_all[frag_s_all > 200]
+                        frag_s_all2 = frag_s_all1[~frag_s_all1.index.duplicated(keep='first')]
+                        frag_final = str(list(frag_s_all2.iloc[:20].index.values))
+                        df1.loc[i, 'frag_DDA'] = frag_final
+                        df1.loc[i, 'MS2_spectra'] = str(frag_s_all2.iloc[:20])
+                df1.to_excel(file_excel)  
+
+
+
+                
+def ultimate_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500,
+                       SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
+                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False,long_rt_split_n = 1,rt_overlap = 1):
+    """
+    Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
+    information on the peaks including retention time, m/z value, intensity, and area.
+
+    Args:
+        ms1 (scan list): generated from sep_scans(file.mzML).
+        profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
+        split_n (int): The number of pieces to split the large dataframe.
+        threshold (int): Threshold for finding peaks.
+        i_threshold (int): Threshold for peak intensity.
+        SN_threshold (float): Signal-to-noise threshold.
+        rt_error_alignment (float, optional): Retention time error alignment threshold.
+        mz_error_alignment (float, optional): m/z error alignment threshold.
+        mz_overlap (float,optional): The m/z overlap (Da) between adjacent sections of data when splitting it.
+        sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+        long_rt_split_n: The number of pieces to split the ms1.
+        rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
+    Returns:
+        pandas.DataFrame: A dataframe with information on the peaks including retention time, m/z value,
+        intensity, and area.
+    """
+    
+    if long_rt_split_n == 1:
+        peak_all = split_peak_picking(ms1, profile=profile, split_n=split_n, threshold=threshold, i_threshold=i_threshold,
+                       SN_threshold=SN_threshold, noise_threshold=noise_threshold, rt_error_alignment=rt_error_alignment,
+                       mz_error_alignment=mz_error_alignment, mz_overlap=mz_overlap, sat_intensity=sat_intensity)
+    else:
+        # Calculate the length of each part
+        total_spectra = len(ms1)
+        part_length = total_spectra // long_rt_split_n
+        overlap_spectra = int(rt_overlap/ (ms1[1].scan_time[0] - ms1[0].scan_time[0]))  # calculate the number of spectra in 1 minute of retention time
+
+        # Split the list into parts
+        parts = []
+        for i in range(long_rt_split_n):
+            start_index = i * part_length - overlap_spectra
+            start_index = max(start_index, 0)  # set start index to 0 if it is less than 0
+            end_index = (i+1) * part_length + overlap_spectra
+            part = ms1[start_index:end_index]
+            parts.append(part)
+
+        # Add any remaining spectra to the last part
+        if end_index < total_spectra:
+            last_part = ms1[end_index:]
+            parts[-1] += last_part
+
+
+
+        parts1 = [ms1[i*part_length:(i+1)*part_length] for i in range(long_rt_split_n)]
+        ranges = []
+        for i, part in enumerate(parts1):
+            rt_start = part[0].scan_time[0]
+            rt_end = part[-1].scan_time[0]
+            range1 = [rt_start,rt_end]
+            ranges.append(range1)
+
+        # start to do peak picking for each part
+        peak_list_all = []
+        for n,part in enumerate(parts):
+            peak_all = split_peak_picking(part, profile=profile, i_threshold=i_threshold,
+                                              SN_threshold=SN_threshold, split_n=split_n, sat_intensity=sat_intensity)
+            peak_all = peak_all[(peak_all['rt']>ranges[n][0])&(peak_all['rt']<=ranges[n][1])]
+            peak_list_all.append(peak_all)
+
+        peak_all = pd.concat(peak_list_all).reset_index(drop = True)
+    return peak_all
+
+
+def first_process(file, company, profile=True, i_threshold=200, SN_threshold=3,
+                  ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n = 1):
+    """
+    Processes HRMS data by performing peak picking and generating a result file.
+
+    Args:
+        file (str): Path to the input file to be processed.
+        company (str): The manufacturer of the instrument used to generate the data (e.g., 'Waters', 'Agilent', etc.).
+        profile (bool): A flag indicating whether or not to perform profiling of chromatographic peaks.
+        SN_threshold (int): The signal-to-noise threshold for peak detection.
+        frag_rt_error (float): The retention time error to use for fragment MS2 analysis.
+        i_threshold (int): The intensity threshold for peak detection.
+        ms2_analysis (bool): A flag indicating whether or not to perform MS2 analysis on fragment peaks.
+        split_n (int): The number of pieces to split the large dataframe.
+        sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+        long_rt_split_n: The number of pieces to split the ms1.
+        rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
+        
+    Returns:
+        None. Instead, the function exports an Excel file with the result information.
+    """
+    mz_round = 4
+    ms1, ms2 = sep_scans(file, company)
+    peak_all = ultimate_peak_picking(ms1, profile=profile, split_n=split_n, i_threshold=i_threshold,
+                       SN_threshold=SN_threshold,  sat_intensity=sat_intensity, long_rt_split_n = long_rt_split_n)
+
+    # 是否分析ms2
+    if len(ms2) == 0:
+        pass
+    else:
+        if ms2_analysis is True:
+            basename_file = os.path.basename(file)
+            if ('control' in basename_file.lower()) | ('blank' in basename_file.lower()) | (
+                    'methanol' in basename_file.lower()) | (
+                    'qaqc' in basename_file.lower()):
+                pass
+            else:
+                print('----------------------------')
+                print('Starting DIA ms2 analysis...')
+                print('----------------------------')
+                peak_all2 = ultimate_peak_picking(ms2, profile=profile, split_n=split_n, i_threshold=i_threshold,
+                       SN_threshold=SN_threshold,  sat_intensity=sat_intensity, long_rt_split_n = long_rt_split_n)
+
+                frag_all = []
+                for i in range(len(peak_all)):
+                    rt = peak_all.loc[i, 'rt']
+                    frag = str(list(peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
+                                              & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
+                        by='intensity', ascending=False)['mz'].values))
+                    frag_all.append(frag)
+                peak_all.loc[:, 'frag_DIA'] = frag_all
+
+        else:
+            pass
+    file_name = os.path.basename(file)
+    peak_selected = identify_isotopes(peak_all)
+    peak_selected = remove_unnamed_columns(peak_selected)
+    peak_selected.to_excel(file.replace('.mzML', '.xlsx'))
+    print('                                                              ')
+    print(f'-------------------------------------------------------------')
+    print(f'Result generated! File name:{file_name}')
+    print('--------------------------------------------------------------')
+    print('                                                              ')
+                
+
 def multi_process(path, company, profile=True, processors=1, p_value=1, ms2_analysis=True, fold_change=0,
-                  area_threshold=200, filter_type=3):
+                  area_threshold=200, filter_type=3, split_n=20, sat_intensity=False,long_rt_split_n = 1):
     """
     This function is to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
        - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
        - ms2_analysis: A Boolean value that indicates whether to perform DIA fragment analysis. Set to True to enable DIA fragment analysis, or False to disable it.
        - fold_change: The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
        - area_threshold: The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
        - filter_type: The type of peak filtering to perform. Set to 1 to only filter for peak area change (based on the maximum of the control group), 2 to filter for both p-value and fold change (treating all controls as a single group), or 3 to filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
-
+       - split_n (int): The number of pieces to split the large dataframe.
+       - sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
 
     returns:
         None.Generate Excel files that summarizes the differences between the control sets and sample sets.
 
     """
     files_mzml = glob(os.path.join(path, '*.mzML'))
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
     # 第一个过程
+    i_threshold = 200
+    SN_threshold = 3,
+    frag_rt_error = 0.02
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
-        pool.apply_async(first_process, args=(file, company, profile, ms2_analysis))
+        pool.apply_async(first_process, args=(file, company, profile, i_threshold, SN_threshold,
+                                              ms2_analysis, frag_rt_error, split_n, sat_intensity,long_rt_split_n))
 
     print('                                                                            ')
     print('============================================================================')
     print('First process started...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
@@ -87,28 +343,29 @@
     if len(path_omitted) == 0:
         pass
     else:
         pool = Pool(processes=processors)
         for file in path_omitted:
             print('Omitted files')
             print(file)
-            pool.apply_async(first_process, args=(file, company, profile, ms2_analysis))
+            pool.apply_async(first_process, args=(file, company, profile, i_threshold, SN_threshold,
+                                                  ms2_analysis, frag_rt_error, split_n, sat_intensity,long_rt_split_n))
         pool.close()
         pool.join()
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
     # 第二个过程
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
-        pool.apply_async(second_process, args=(file, ref_all, company, profile))
+        pool.apply_async(second_process, args=(file, ref_all, company, profile,long_rt_split_n))
     print('                                                                            ')
     print('============================================================================')
     print('Second process started...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
     pool.join()
@@ -162,15 +419,15 @@
     Args:
        path (str): The path of the mzML file.
        company (str): The instrument company name. Currently, supports 'Waters', 'Agilent', 'Thermo' or 'AB'.
 
     Returns:
         Tuple: A tuple of two lists containing MS1 and MS2 scans respectively.
     """
-    if company == 'Waters':
+    if company.lower() == 'waters':
         # create a pymzml Reader object
         run = pymzml.run.Reader(path)
         ms1, ms2 = [], []
         # iterate over each scan in the mzML file
         for scan in tqdm(run, desc='Separating MS1 and MS2'):
             # extract function value from the scan's id_dict attribute
             if scan.id_dict['function'] == 1:
@@ -423,15 +680,15 @@
             if enable_progress_bar:
                 print('Performing alignment for single file...', end='')
 
             peak_p = np.array([peak_all.rt.values, peak_all.mz.values]).T
             indice = [
                 peak_all[
                     (peak_all.mz > peak_p[i][1] - mz_error_alignment) & (
-                                peak_all.mz < peak_p[i][1] + mz_error_alignment) &
+                            peak_all.mz < peak_p[i][1] + mz_error_alignment) &
                     (peak_all.rt > peak_p[i][0] - rt_error_alignment) & (
                             peak_all.rt < peak_p[i][0] + rt_error_alignment)].index[-1] for
                 i in range(len(peak_p))]
             indice1 = np.array(list(set(indice)))
             peak_all = peak_all.loc[indice1, :].sort_values(by='intensity', ascending=False).reset_index(drop=True)
             t1 = time.time()
             if enable_progress_bar:
@@ -646,15 +903,15 @@
     x_new = np.linspace(x.min(), x.max(), num=num_points, endpoint=True)
     y_new = f(x_new)
     return x_new, y_new
 
 
 def split_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500,
                        SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
-                       mz_error_alignment=0.015, mz_overlap=1,sat_intensity = False):
+                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
     Args:
         ms1 (scan list): generated from sep_scans(file.mzML).
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
@@ -703,15 +960,15 @@
             data=ms1[i].i, index=ms1[i].mz.round(4), name=round(ms1[i].scan_time[0], 3)) for i in
             range(len(ms1))}
         data = [pd.Series(data=v.values, index=v.index.values.round(3), name=v.name) for k, v in
                 raw_info_centroid.items()]
 
     t2 = time.time()
     time1 = round(t2 - t1, 0)
-    print(f'{time1}')
+    print(f'\r *** Loading data successfully: {time1} s')
 
     # 开始分割
     # 定义变量名称
     all_data = []
     for j in range(split_n):
         name = 'a' + str(j + 1)
         locals()[name] = []
@@ -743,69 +1000,81 @@
                                     rt_error_alignment=rt_error_alignment,
                                     mz_error_alignment=mz_error_alignment, enable_progress_bar=False, alignment=False)
             all_peak_all.append(peak_all)
 
     peak_all = pd.concat(all_peak_all).sort_values(by='intensity', ascending=False).reset_index(drop=True)
 
     # 做alignment
-    print(f'Single file alignment...')
+    print('\r Single file alignment...', end='')
+    t1 = time.time()
     peak_p = np.array([peak_all.rt.values, peak_all.mz.values]).T
     indice = [
         peak_all[
             (peak_all.mz > peak_p[i][1] - mz_error_alignment) & (peak_all.mz < peak_p[i][1] + mz_error_alignment) &
             (peak_all.rt > peak_p[i][0] - rt_error_alignment) & (
                     peak_all.rt < peak_p[i][0] + rt_error_alignment)].index[-1] for
         i in range(len(peak_p))]
     indice1 = np.array(list(set(indice)))
     peak_all = peak_all.loc[indice1, :].sort_values(by='intensity', ascending=False).reset_index(drop=True)
+    t2 = time.time()
+    t_ = round(t2 - t1, 1)
+    print(f'\r *** Single file alignment finished: {t_} s')
 
     # 对同位素丰度进行记录
-    print(f'Recording isotope information...')
-
+    print('\r Recording isotope information...', end='')
     raw_info_rts = [v.name for k, v in raw_info_centroid.items()]
     rts = peak_all.rt.values
     mzs = peak_all.mz.values
     rt_keys = [raw_info_rts[argmin(abs(np.array(raw_info_rts) - i))] for i in rts]  # 基于上述rt找到ms的时间索引
 
     iso_info = [str(isotope_distribution(raw_info_centroid[rt_keys[i]], mzs[i])) for i in range(len(mzs))]
     peak_all['iso_distribution'] = iso_info
+    t3 = time.time()
+    t_ = round(t3 - t2, 1)
+    print(f'\r *** Recording isotope information finished: {t_} s')
 
     # 更新质量数据
-    print(f'Checking/Optimizing mass...')
+    print('Checking/Optimizing mass...', end='')
     if profile is True:
         spec1 = [raw_info_profile[i] for i in rt_keys]  # 获得ms的spec
         mz_result = np.array(
             [list(evaluate_ms(target_spec1(spec1[i], mzs[i], width=0.04).copy(), mzs[i])) for i in range(len(mzs))]).T
         mz_obs, mz_opt, resolution = mz_result[0], mz_result[2], mz_result[4]
         mz_opt = [mz_opt[i] if abs(mzs[i] - mz_opt[i]) < 0.02 else mzs[i] for i in range(len(mzs))]  # 去掉偏差大的矫正结果
 
         peak_all.loc[:, ['mz', 'mz_opt', 'resolution']] = np.array([mz_obs, mz_opt, resolution.astype(int)]).T
     else:
         spec1 = [raw_info_centroid[i] for i in rt_keys]  # 获得ms的spec
         target_spec = [spec1[i][(spec1[i].index > mzs[i] - 0.015) & (spec1[i].index < mzs[i] + 0.015)] for i in
                        range(len(spec1))]
         mzs_obs = [target_spec[i].index.values[[np.argmax(target_spec[i].values)]][0] for i in range(len(target_spec))]
         peak_all['mz'] = mzs_obs
-    
+    t4 = time.time()
+    t_ = round(t4 - t3, 1)
+    print(f'\r *** Checking/Optimizing mass finished: {t_} s')
+
     # 如果担心饱和质量不准，使用sat_intensity 更新质量
-    if isinstance(sat_intensity, (int, float, complex)):
-        for j in tqdm(range(len(peak_all)),desc = 'Optimize m/z based on sat_intensity:'):
-            mz = peak_all.loc[j,'mz']
-            rt = peak_all.loc[j,'rt']
-            intensity = peak_all.loc[j,'intensity']
+    if (sat_intensity is False) | (sat_intensity is None):
+        pass
+    else:
+        for j in tqdm(range(len(peak_all)), desc='Optimize m/z based on sat_intensity'):
+            mz = peak_all.loc[j, 'mz']
+            rt = peak_all.loc[j, 'rt']
+            intensity = peak_all.loc[j, 'intensity']
             if intensity > sat_intensity:
-                for k,v in raw_info_profile.items(): 
-                    if k>rt: # find the time
+                for k, v in raw_info_profile.items():
+                    if k > rt:  # find the time
                         s1 = raw_info_profile[k]
-                        new_spec1 =target_spec1(s1,mz,0.2) # cut the spectrum in a certain range. 1 m/z in this case.
-                        peak_index,*_ = peak_finding(new_spec1.values)
+                        new_spec1 = target_spec1(s1, mz,
+                                                 0.2)  # cut the spectrum in a certain range. 1 m/z in this case.
+                        peak_index, *_ = scipy.signal.find_peaks(new_spec1.values)
                         if max(new_spec1.values[peak_index]) < sat_intensity:
-                            insat_mz_obs, error1, insat_mz_opt, error2, resolution = evaluate_ms(new_spec1,mz)
-                            peak_all.loc[j,'mz'] =insat_mz_obs
-                            peak_all.loc[j,'mz_opt'] = insat_mz_opt
+                            insat_mz_obs, error1, insat_mz_opt, error2, resolution = evaluate_ms(new_spec1, mz)
+                            peak_all.loc[j, 'mz'] = insat_mz_obs
+                            peak_all.loc[j, 'mz_opt'] = insat_mz_opt
                             break
     return peak_all
 
 
 def remove_unnamed_columns(df):
     """
     Remove any columns in the input DataFrame that are named 'Unnamed:*'.
@@ -816,71 +1085,14 @@
     Returns:
         pandas.DataFrame: A new DataFrame with the 'Unnamed:*' columns removed.
     """
     unnamed_columns = [col for col in df.columns if col.startswith('Unnamed:')]
     return df.drop(columns=unnamed_columns).reset_index(drop=True)
 
 
-def first_process(file, company, profile=True, i_threshold=200, SN_threshold=3,
-                  ms2_analysis=True, frag_rt_error=0.02):
-    """
-    Processes HRMS data by performing peak picking and generating a result file.
-
-    Args:
-        file (str): Path to the input file to be processed.
-        company (str): The manufacturer of the instrument used to generate the data (e.g., 'Waters', 'Agilent', etc.).
-        profile (bool): A flag indicating whether or not to perform profiling of chromatographic peaks.
-        SN_threshold (int): The signal-to-noise threshold for peak detection.
-        frag_rt_error (float): The retention time error to use for fragment MS2 analysis.
-        i_threshold (int): The intensity threshold for peak detection.
-        ms2_analysis (bool): A flag indicating whether or not to perform MS2 analysis on fragment peaks.
-
-    Returns:
-        None. Instead, the function exports an Excel file with the result information.
-    """
-    mz_round = 4
-    ms1, ms2 = sep_scans(file, company)
-    peak_all = split_peak_picking(ms1, profile=profile, i_threshold=i_threshold,
-                                  SN_threshold=SN_threshold)
-    if len(ms2) == 0:
-        pass
-    else:
-        if ms2_analysis is True:
-            basename_file = os.path.basename(file)
-            if ('control' in basename_file.lower()) | ('blank' in basename_file.lower()) | (
-                    'methanol' in basename_file.lower()) | (
-                    'qaqc' in basename_file.lower()):
-                pass
-            else:
-                print('----------------------------')
-                print('Starting DIA ms2 analysis...')
-                print('----------------------------')
-                peak_all2 = split_peak_picking(ms2, profile=profile, i_threshold=i_threshold, SN_threshold=SN_threshold)
-
-                frag_all = []
-                for i in range(len(peak_all)):
-                    rt = peak_all.loc[i, 'rt']
-                    frag = str(list(peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
-                                              & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
-                        by='intensity', ascending=False)['mz'].values))
-                    frag_all.append(frag)
-                peak_all.loc[:, 'frag_DIA'] = frag_all
-
-        else:
-            pass
-    file_name = os.path.basename(file)
-    peak_selected = identify_isotopes(peak_all)
-    peak_selected = remove_unnamed_columns(peak_selected)
-    peak_selected.to_excel(file.replace('.mzML', '.xlsx'))
-    print('                                                              ')
-    print(f'-------------------------------------------------------------')
-    print(f'Result generated! File name:{file_name}')
-    print('--------------------------------------------------------------')
-    print('                                                              ')
-
 
 def identify_isotopes(cmp, iso_error=0.005):
     """
     Identify isotopes and adducts in the unique compounds dataframe based on their mass-to-charge ratio (m/z) and retention time (rt).
 
     Args:
         cmp: pandas DataFrame of unique compounds
@@ -1106,40 +1318,99 @@
 
     # Combine reference pairs
     final_reference_pairs = np.vstack([reference_pairs, omitted_reference_pairs])
 
     return final_reference_pairs
 
 
-def second_process(file, ref_all, company, profile=True):
+def second_process(file, ref_all, company, profile=True,long_rt_split_n = 1):
     """
     This function will use the reference rt&mz pair, and obtain the peak area at specific rt & mz
     Args:
         profile: True or False
         file: single file to process
         ref_all: all reference peaks
         company: e.g., 'Waters', 'Agilent',etc,
     returns:
         export to files
+ 
     """
     ms_round = 4
     ms1, ms2 = sep_scans(file, company)
 
     name1 = os.path.basename(file).split('.')[0]
-    final_result = peak_checking_area_split(ref_all, ms1,
-                                            name1, profile=profile)
+    final_result = ultimate_checking_area(ref_all, ms1, name1, profile=profile,
+                                          rt_overlap=1, long_rt_split_n = long_rt_split_n)
     final_result.to_excel(file.replace('.mzML', '_final_area.xlsx'))
 
     print('                                                              ')
     print('--------------------------------------------------------------')
     print(f'Final area files have been created: {os.path.basename(file)}')
     print('--------------------------------------------------------------')
     print('                                                              ')
 
 
+    
+def ultimate_checking_area(ref_all, ms1, name1, profile = True, 
+                           split_n = 20, rt_overlap=1, long_rt_split_n = 4):
+    """
+    Based on peak reference, intergrate peak are for each reference m/z and retention time pair.
+    
+    Args:
+        ref_all: reference m/z and retention time pair
+        ms1: generated from sep_scans(file.mzML).
+        name1: file name.
+        profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
+        split_n: The number of pieces to split the large dataframe.
+        long_rt_split_n: The number of pieces to split the ms1.
+        rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
+    return:
+        The final areas for reference m/z and retention time pair.
+    """
+    
+    if long_rt_split_n == 1:
+        final_area = peak_checking_area_split(ref_all, ms1, name1, profile=True, split_n=split_n, noise_threshold=0)
+       
+    else:
+        # Calculate the length of each part
+        total_spectra = len(ms1)
+        part_length = total_spectra // long_rt_split_n
+        overlap_spectra = int(rt_overlap/ (ms1[1].scan_time[0] - ms1[0].scan_time[0]))  # calculate the number of spectra in 1 minute of retention time
+
+        # Split the list into parts
+        parts = []
+        for i in range(long_rt_split_n):
+            start_index = i * part_length - overlap_spectra
+            start_index = max(start_index, 0)  # set start index to 0 if it is less than 0
+            end_index = (i+1) * part_length + overlap_spectra
+            part = ms1[start_index:end_index]
+            parts.append(part)
+
+        # Add any remaining spectra to the last part
+        if end_index < total_spectra:
+            last_part = ms1[end_index:]
+            parts[-1] += last_part
+
+        parts1 = [ms1[i*part_length:(i+1)*part_length] for i in range(long_rt_split_n)]
+        mz_list = [round(part[0].scan_time[0],3) for part in parts1]
+        mz_list.append(parts1[-1][-1].scan_time[0])
+        ranges = [[mz_list[i], mz_list[i+1]] for i in range(len(mz_list)-1)]
+        # start to split ref_all
+        ref_all = remove_unnamed_columns(ref_all)
+        ref_all_parts = [ref_all[(ref_all['rt']>=ranges1[0])&(ref_all['rt']<ranges1[1])] for ranges1 in ranges]
+        
+        # start to collect each peak_all
+        peak_area_all = []
+        for i in range(len(parts)):
+            each_peak_area = peak_checking_area_split(ref_all_parts[i],parts[i], '', profile = profile,split_n=split_n)
+            peak_area_all.append(each_peak_area)
+        final_area = pd.concat(peak_area_all)
+        final_area.columns = [name1]
+    return final_area    
+    
 def peak_checking_area(ref_all, df1, name):
     """
     Obtain the area for each rt&mz pair in df1
     :param ref_all:  peak_reference
     :param df1: dataframe df1
     :param name: name
     :return: new_dataframe
@@ -1225,17 +1496,20 @@
         all_peak_ref.append(locals()[name])
 
     # 获取所有area
     area_all = []
     for i in tqdm(range(split_n), desc='Integrating peak area:'):
         peak_ref1 = all_peak_ref[i]
         df1 = pd.concat(all_data[i], axis=1)
-        df1 = df1.fillna(0)
-        df_area = peak_checking_area(peak_ref1, df1, 'split')
-        area_all.append(df_area)
+        if len(df1) == 0:
+            pass
+        else:
+            df1 = df1.fillna(0)
+            df_area = peak_checking_area(peak_ref1, df1, 'split')
+            area_all.append(df_area)
 
     # 合成所有的area
     final_df = pd.concat(area_all)
     final_df.columns = [name1]
     return final_df
 
 
@@ -2488,27 +2762,33 @@
         num (int): The number of peaks in the output.
 
     Returns:
         mz_iso (np.array): The mass-to-charge ratio (m/z) values for the isotopes.
         i_iso (np.array): The relative intensities of the isotopes.
 
     Example:
-        >>> formula_to_distribution('C13H13N3', '+H', 3)
+        formula_to_distribution('C13H13N3', '+H', 3)
         (array([198.1072, 199.11  , 200.1129]), array([100. ,  41.5,  10.5]))
     """
 
     f = Formula(formula)
     a = f.spectrum()
     mz_iso, i_iso = np.array([a for a in a.values()]).T
     i_iso = i_iso / i_iso[0] * 100
     if adducts == '+H':
         mz_iso += 1.00727647
     elif adducts == '-H':
         mz_iso -= 1.00727647
-    mz_iso = mz_iso.round(4)
+    elif adducts == '-':
+        mz_iso += 0.00054858
+    elif adducts == '+':
+        mz_iso -= 0.00054858
+    else:
+        pass
+    mz_iso = mz_iso.round(6)
     i_iso = i_iso.round(1)
     s1 = pd.Series(data=i_iso, index=mz_iso).sort_values(ascending=False)
     return s1.index.values[:num], s1.values[:num]
 
 
 def isotope_matching(iso_info, formula):
     """
@@ -2994,15 +3274,15 @@
         high_mz = mz * (1 + error * 1e-6)
         for scan in df:
             mz_all = scan.mz
             i_all = scan.i
             rt1 = scan.scan_time[0]
             rt.append(rt1)
             index_e = np.where((mz_all <= high_mz) & (mz_all >= low_mz))
-            eic1 = 0 if len(index_e[0]) == 0 else i_all[index_e[0]].sum()
+            eic1 = 0 if len(index_e[0]) == 0 else i_all[index_e[0]].max()
             intensity.append(eic1)
     else:
         rt, intensity = None, None
     return rt, intensity
 
 
 """
@@ -3046,15 +3326,15 @@
         A pandas DataFrame with the average final areas for each sample set.
 
     Raises:
         FileNotFoundError: If no files are found in the specified path.
         ValueError: If no final area files are found in the specified path.
 
     Examples:
-        >>> omics_final_area('path/to/final_area_files', ['SampleSet1', 'SampleSet2'])
+        omics_final_area('path/to/final_area_files', ['SampleSet1', 'SampleSet2'])
              SampleSet1  SampleSet2
         0            10          20
         1            12          25
         2            15          30
     """
 
     # Get a list of all files in the specified path that have the name 'final_area' and an .xlsx extension.
@@ -3415,14 +3695,296 @@
 
 
 """
 ========================================================================================================
 4. FT-ICRMS data processing
 ========================================================================================================
 """
+def FT_ICRMS_process(path, formula='C50H60O50N1S1', mz_range=None, peak_threshold=6, error=1, iso_error=0.0003,
+             iso_fold_change=2, mode='neg'):
+    
+    """
+    Process FT_ICRMS data and find possible formula.
+    
+    Args:
+        file_path (str): File path for profile raw data; supported formats: .xy, .csv, .xlsx
+        formula (str): Formula range for prediction (default: 'C50H60O50N1S1')
+        mz_range (list): m/z range for prediction (default: None, which sets it to [200, 800])
+        peak_threshold (int): Similar to signal to noise (default: 6)
+        error (int): m/z error for formula prediction, unit: part per million (ppm) (default: 1)
+        iso_error (float): m/z error for isotope assignment, unit: Dalton (default: 0.0003)
+        iso_fold_change (int): The peak intensity/isotope intensity (default: 2)
+        mode (str): 'pos' for positive, 'neg' for negative (default: 'neg')
+
+    Returns:
+        DataFrame: A dataframe with formula prediction
+    """
+    
+    if mz_range is None:
+        mz_range = [200, 800]
+    atom_mass_table1 = pd.Series(
+        data={'C': 12.000000, 'Ciso': 13.003355, 'N': 14.003074, 'Niso': 15.000109, 'O': 15.994915, 'H': 1.007825,
+              'Oiso': 17.999159, 'F': 18.998403, 'K': 38.963708, 'P': 30.973763, 'Cl': 34.968853,
+              'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770, 'Si': 27.976928,
+              'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
+              'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486
+              })
+    
+    def formula_sep(formula):
+        """
+        Transform formula to atoms list and atoms number list.
+        :param formula: e.g., 'C13H13N3'
+        """
+        a = re.findall('[A-Z][a-z]*|\\d+', formula)
+        b = {}
+        for i in range(len(a)):
+            try:
+                eval(a[i])
+            except NameError:
+                try:
+                    b[a[i]] = eval(a[i + 1])
+                except NameError:
+                    b[a[i]] = 1
+        c = pd.Series(b)
+        atoms = list(c.index)
+        atom_n1 = list(c.values)
+        atom_n = []
+        for num in atom_n1:
+            atom_n.append([0, num])
+        return atoms, atom_n
+
+
+    # 查找同位素的函数
+    def find_isotopes(data1, atom_mass_table2, error1=0.0003, iso_fold_change1=5):
+
+        Ciso = atom_mass_table2['Ciso'] - atom_mass_table2['C']
+        Niso = atom_mass_table2['Niso'] - atom_mass_table2['N']
+        Oiso = atom_mass_table2['Oiso'] - atom_mass_table2['O']
+        Siso = atom_mass_table2['Siso'] - atom_mass_table2['S']
+        Ciso_all = []
+        Niso_all = []
+        Oiso_all = []
+        Siso_all = []
+
+        for mz1 in tqdm(data1['m/z'].values, desc='Finding isotopes'):
+            mz_i = data1[data1['m/z'] == mz1]['i'].values[0]
+            mz_s = data1['m/z'] - mz1
+            C = np.where((mz_s < Ciso + error1) & (mz_s > Ciso - error1))
+            C_i = data1.loc[C[0]]['i']
+            N = np.where((mz_s < Niso + error1) & (mz_s > Niso - error1))
+            N_i = data1.loc[N[0]]['i']
+            O = np.where((mz_s < Oiso + error1) & (mz_s > Oiso - error1))
+            O_i = data1.loc[O[0]]['i']
+            S = np.where((mz_s < Siso + error1) & (mz_s > Siso - error1))
+            S_i = data1.loc[S[0]]['i']
+
+            if len(C[0]) > 0:
+                if mz_i > iso_fold_change1 * C_i.values[0]:
+                    Ciso_all.append(int(C[0]))
+            if len(N[0]) > 0:
+                if mz_i > iso_fold_change1 * N_i.values[0]:
+                    Niso_all.append(int(N[0]))
+            if len(O[0]) > 0:
+                if mz_i > iso_fold_change1 * O_i.values[0]:
+                    Oiso_all.append(int(O[0]))
+            if len(S[0]) > 0:
+                if mz_i > iso_fold_change1 * S_i.values[0]:
+                    Siso_all.append(int(S[0]))
+        return np.array(Ciso_all), np.array(Niso_all), np.array(Oiso_all), np.array(Siso_all)
+
+    # 生成所有可能的formula
+    def generate_formula_df(mz_range1, atoms1, atom_n1, mode1='neg'):
+        """
+        generate possible formula set for mz range
+        """
+        if mode1 == 'pos':
+            e = atom_mass_table1['e'] * -1
+        elif mode1 == 'neg':
+            e = atom_mass_table1['e']
+        else:
+            print('mode set as positive')
+            e = atom_mass_table1['e'] * -1
+        pattern = []
+        for n in range(atom_n1[0][0], atom_n1[0][1] + 2):
+            i_mz_remain = mz - atom_mass_table1[atoms1[0]] * n  # i还剩多少质量
+            j1 = int(np.floor(i_mz_remain / atom_mass_table1[atoms1[1]])) if int(
+                np.floor(i_mz_remain / atom_mass_table1[atoms1[1]])) < atom_n1[1][1] else atom_n1[1][1]
+            for j in range(atom_n1[1][0], j1 + 2):
+                j_mz_remain = mz - atom_mass_table1[atoms1[0]] * n - atom_mass_table1[atoms1[1]] * j  # j还剩多少质量
+                k1 = int(np.floor(j_mz_remain / atom_mass_table1[atoms1[2]])) if int(
+                    np.floor(j_mz_remain / atom_mass_table1[atoms1[2]])) < atom_n1[2][1] else atom_n1[2][1]
+                for k in range(atom_n1[2][0], k1 + 2):
+                    for l in range(atom_n1[3][0], atom_n1[3][1] + 2):
+                        for m in range(atom_n1[4][0], atom_n1[4][1] + 2):
+                            pattern.append([n, j, k, l, m])
+        pattern_df1 = pd.DataFrame(pattern, columns=atoms1)
+        pattern_df1['m/z_exp'] = (pattern_df1[atoms1[0]] * atom_mass_table1[atoms1[0]] +
+                                  pattern_df1[atoms1[1]] * atom_mass_table1[atoms1[1]] +
+                                  pattern_df1[atoms1[2]] * atom_mass_table1[atoms1[2]] +
+                                  pattern_df1[atoms1[3]] * atom_mass_table1[atoms1[3]] +
+                                  pattern_df1[atoms1[4]] * atom_mass_table1[atoms1[4]] + e)
+        pattern_df1 = pattern_df1.sort_values(by='m/z_exp')
+        pattern_df1 = pattern_df1[
+            (pattern_df1['m/z_exp'] > mz_range1[0]) & (pattern_df1['m/z_exp'] < mz_range1[1]) & (pattern_df1['C'] >= 3)
+            & (pattern_df1['H'] >= 1) & (pattern_df1['O'] >= 1)].reset_index(drop=True)
+        return pattern_df1
+
+    # 开始匹配
+    def formula_match(data1, pattern_df1, error1=1.05, iso_error1=0.0003, iso_fold_change1=5):
+        # 匹配同位素
+        def iso_match(data2, Ciso_index2, pattern_df2, marker, error2=1.05):
+            final_data = []
+            for n in tqdm(range(len(data2.loc[Ciso_index2])), desc=f'matching {marker} isotope'):
+                iso = marker[0] + 'iso'
+                mz1 = data2.loc[Ciso_index2].iloc[n, 0] - atom_mass_table1[iso]
+                df2 = pattern_df2[(pattern_df2['m/z_exp'] < mz1 * (1 + error2 * 1e-6)) & (
+                        pattern_df2['m/z_exp'] > mz1 * (1 - error2 * 1e-6))].copy()
+                df2 = df2[(df2['C'] >= df2['O'] / 1.2) & (df2['C'] >= df2['H'] / 2.5)]  # 筛选条件
+                if len(df2) == 0:
+                    pass
+                else:
+                    df2['m/z_obs'] = data2.loc[Ciso_index2].iloc[n, 0]
+                    df2['intensity'] = data2.loc[Ciso_index2].iloc[n, 1]
+                    df2['error(ppm)'] = ((df2['m/z_exp'] - mz1) / mz1 * 1 * 1e6).round(4)
+                    df2['error_abs'] = df2['error(ppm)'].abs()
+                    df2['hetero'] = df2['N'] + df2['S']
+                    df2['isotope'] = marker
+                    s1 = df2.sort_values(by='hetero').iloc[0]
+                    s1.loc[marker[0]] += 1
+                    s1.loc['m/z_exp'] += atom_mass_table1[iso]
+                    final_data.append(s1)
+            if len(final_data) == 0:
+                return None
+            else:
+                return pd.concat(final_data, axis=1).T
+
+        # 匹配普通峰
+        def normal_match(data3, Ciso_index3, pattern_df3, marker, error3=1.05):
+            final_data = []
+            for i1 in tqdm(range(len(data3.loc[Ciso_index3])), desc='matching normal peaks'):
+                mz1 = data3.loc[Ciso_index3].iloc[i1, 0]
+                df2 = pattern_df3[(pattern_df3['m/z_exp'] < mz1 * (1 + error3 * 1e-6)) & (
+                        pattern_df3['m/z_exp'] > mz1 * (1 - error3 * 1e-6))].copy()
+                df2 = df2[(df2['C'] >= df2['O'] / 1.2) & (df2['C'] >= df2['H'] / 2.5)]  # 筛选条件
+                if len(df2) == 0:
+                    pass
+                else:
+                    df2['m/z_obs'] = mz1
+                    df2['intensity'] = data3.loc[Ciso_index3].iloc[i1, 1]
+                    df2['error(ppm)'] = ((df2['m/z_exp'] - mz1) / mz1 * 1 * 1e6).round(4)
+                    df2['error_abs'] = df2['error(ppm)'].abs()
+                    df2['hetero'] = df2['N'] + df2['S']
+                    df2['isotope'] = marker
+                    s1 = df2.sort_values(by='hetero').iloc[0]
+                    final_data.append(s1)
+            if len(final_data) == 0:
+                return None
+            else:
+                return pd.concat(final_data, axis=1).T
+
+        # 开始处理
+        Ciso_index, Niso_index, Oiso_index, Siso_index = find_isotopes(data1, atom_mass_table1, iso_error1,
+                                                                       iso_fold_change1)
+
+        all_iso_index = np.concatenate([Ciso_index, Niso_index, Oiso_index, Siso_index])
+        peak_no_iso = np.delete(data1.index.values, all_iso_index.astype(int))
+        # 处理同位素
+        data_to_concat = []
+        if len(Ciso_index) != 0:
+            Ciso_df = iso_match(data1, Ciso_index, pattern_df1, 'C13', error2=error1)
+            data_to_concat.append(Ciso_df)
+        if len(Niso_index) != 0:
+            Niso_df = iso_match(data1, Niso_index, pattern_df1, 'N15', error2=error1)
+            data_to_concat.append(Niso_df)
+        if len(Oiso_index) != 0:
+            Oiso_df = iso_match(data1, Oiso_index, pattern_df1, 'O18', error2=error1)
+            data_to_concat.append(Oiso_df)
+        if len(Siso_index) != 0:
+            Siso_df = iso_match(data1, Siso_index, pattern_df1, 'S34', error2=error1)
+            data_to_concat.append(Siso_df)
+        # 处理其他
+        peak_no_iso_df = normal_match(data1, peak_no_iso, pattern_df1, '', error3=error1)
+        data_to_concat.append(peak_no_iso_df)
+
+        return pd.concat(data_to_concat)
+
+    # 读取数据
+    raw_data = pd.read_csv(path, delimiter=' ', names=['m/z', 'i'])
+
+    # 分割分子式
+    atoms, atom_n = formula_sep(formula)
+    mz = mz_range[1]
+    for i in range(len(atoms)):
+        num = int(np.floor(mz / atom_mass_table1[atoms[i]])) if int(np.floor(mz / atom_mass_table1[atoms[i]])) < \
+                                                                atom_n[i][1] else atom_n[i][1]
+        atom_n[i][1] = num
+
+    # 找到峰
+    eic = raw_data.loc[:, 'i']
+    index, index_left, index_right = peak_finding(eic, threshold=peak_threshold)
+    data = raw_data.loc[index, :].reset_index(drop=True)
+    background = np.mean(eic) * 2.5
+    # 生成所有可能的formula
+    pattern_df = generate_formula_df(mz_range, atoms, atom_n)
+
+    # 开始匹配
+    final_result = formula_match(data, pattern_df, error1=error, iso_error1=iso_error, iso_fold_change1=iso_fold_change)
+    final_result = final_result.sort_values(by='m/z_obs').reset_index(drop=True)
+
+    # 矫正formula,把C13H13O3N0S0转成C13H13N3
+    def process_formula(formula4, mode4):
+        b = re.findall('[A-Z][a-z]*|\\d+', formula4)
+        for j in range(len(b)):
+            if (b[j] == 'H') & (mode4 == 'pos'):
+                b[j + 1] = str(eval(b[j + 1]) - 1)
+            elif (b[j] == 'H') & (mode4 == 'neg'):
+                b[j + 1] = str(eval(b[j + 1]) + 1)
+        c = ''
+        for j in range(len(b)):
+            if b[j].isdigit():
+                if eval(b[j]) == 1:
+                    c += b[j - 1]
+                elif eval(b[j]) > 1:
+                    c += b[j - 1]
+                    c += b[j]
+        return c
+
+    # 把所有formula整合
+    formula1 = final_result.columns[0] + final_result.iloc[:, 0].astype(str)
+    for i in range(1, 5):
+        formula1 += final_result.columns[i] + final_result.iloc[:, i].astype(str)
+    final_result['formula'] = formula1.apply(process_formula, mode4=mode)
+
+    # 计算其他参数
+    final_result['S/N'] = (final_result['intensity'] / background).astype(float).round(2)
+    final_result['O/C'] = (final_result['O'] / final_result['C']).astype(float).round(3)
+
+    if mode == 'pos':
+        x = 1
+    elif mode == 'neg':
+        x = -1
+    else:
+        x = 1
+        print('mode set as positive')
+    final_result['H/C'] = ((final_result['H'] - x) / final_result['C']).astype(float).round(3)
+    final_result['DBE'] = 1 + 0.5 * (2 * final_result['C'] - (final_result['H'] - x) + final_result['N'])
+    final_result['NOSC'] = 4 - (4 * final_result['C'] + (final_result['H'] - x)
+                                - 3 * final_result['N'] - 2 * final_result['O'] - 2 * final_result['S']) / final_result[
+                               'C']
+    final_result['NOSC'] = final_result['NOSC'].astype(float).round(3)
+    AI_denominator = final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - final_result['N']
+    AI_numerator = 1 + final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - 0.5 * (final_result['H'] - x)
+    AI = AI_numerator / (AI_denominator.sort_values() + 1 * 1e-6)
+    final_result['AI'] = AI
+    return final_result
+
+
+
+
+
 
 """
 ========================================================================================================
 5. other functions
 ========================================================================================================
 """
```

## Comparing `pyhrms-0.5.5/pyhrms/__init__.py` & `pyhrms-0.5.6/pyhrms/__init__.py`

 * *Files identical despite different names*

## Comparing `pyhrms-0.5.5/pyhrms.egg-info/PKG-INFO` & `pyhrms-0.5.6/pyhrms.egg-info/PKG-INFO`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.5.5
+Version: 0.5.6
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,18 +22,18 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-Mar.23.2023: pyhrms 0.5.5 new features:
-
-    * add sat_intensity functions paramter in peak_picking fuction. The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
+Mar.23.2023: pyhrms 0.5.6 new features:
 
+    * Split in different direction (RT&m/z)
+    * Can process FT_ICRMS data
 
 
 pyhrms can be installed and import as following:
 
 .. code-block:: python
 
     pip install pyhrms
```

